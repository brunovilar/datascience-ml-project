---
title: "Practical Machine Learning -- Course Project"
author: "Bruno Vilar"
date: "22-08-2015"
output:
  html_document:
    keep_md: yes
    toc: yes
---

```{r, echo=FALSE, message=FALSE}
library(caret)
library(DT)
```


# Introduction

This report presents the development of a model to perform Human Activity Prediction based on [HAR dataset](http://groupware.les.inf.puc-rio.br/har). This task is part of the [Practical Machine Learning](https://www.coursera.org/course/predmachlearn) course, from the [Data Science Specialization](https://www.coursera.org/specialization/jhudatascience/1) on [Coursera](https://www.coursera.org).

---

# Loading and Separating Data

In the context of the course, two files were provided: *pml-training.csv* and *pml-testing.csv*.

```{r}
labeledDataset = read.csv(file = "./data/pml-training.csv", stringsAsFactors=FALSE)
unlabledDataset = read.csv(file = "./data/pml-testing.csv")
```

As we dont have access to the labels of the second dataset, we need to creat a validation set from the *pml-training.csv*. The test set will be automatically extracted by [Caret package](https://topepo.github.io/caret/) during repeated 10 fold cross validation. The *pml-testing.csv* is considered an alternative validation, performed by Coursera's website. For this purpose, we divided the training into training and validation sets, respectivelly, with 75% and 25% of the rows.

```{r}
courseraValidation = unlabledDataset

trainingIndex = createDataPartition(labeledDataset$classe, p = 0.75, list = FALSE)[, 1]
training = labeledDataset[trainingIndex,]
validation = labeledDataset[-trainingIndex,]

df = data.frame(Training=nrow(training), Validation=nrow(validation))
datatable(df, rownames=FALSE, options = list( searching = FALSE, pageLength = 5 ))
```

---

# Data Preparation

As we want to avoid gaining any information from the validation set, only the training set will be analyzed. We start by analyzing its structure. There are columns with a high number of NA and incorrect values (empty or "#DIV/0!"). The list can be seen below.

```{r, results='markup', cache=FALSE}
NAs = sapply(colnames(training), function(item){
    sum(as.numeric(is.na(training[, item])))/nrow(training)
})

blankOrWrong = sapply(colnames(training), function(item){
    sum(as.numeric((training[, item]) == "" | training[, item] == "#DIV/0!"))/nrow(training)
})

results = data.frame(Column=names(NAs), NAs=NAs, BlankOrWrong=blankOrWrong)

datatable(results, rownames=FALSE, options = list( searching = TRUE, pageLength = 15 ))
```

Due to the high number of NAs and fields with blank or wrong values (more than 97%) in some columns, is not viable to impute the values. Based on this, we remove the columns from all datasets, including validation and test sets, without looking their content.

```{r}
columnsToRemove = c( row.names(results[which(results$NAs > 0.9), ]), row.names(results[which(results$BlankOrWrong > 0.9), ]))

training = training[, !(names(training) %in% columnsToRemove)]
validation = validation[, !(names(validation) %in% columnsToRemove)]
courseraValidation = courseraValidation[, !(names(courseraValidation) %in% columnsToRemove)]
```

Originally we thought in using the timestamp information, extracting features such as week day and hour of the day as a indicative of the users' routines. However, the number of values are limited (as shown below) and it is better for the model to depend only on sensors to recognize the human activities.

```{r}
dateTime = strptime(as.character(training$cvtd_timestamp), "%d/%m/%Y %H:%M")

df = data.frame(Columns=c("Month", "Day of Month", "Day of Week", "Hour"),
                Values=c(
    paste(unique(dateTime$mon+1), collapse = ", "),
    paste(unique(dateTime$mday), collapse = ", "),
    paste(unique(dateTime$wday), collapse = ", "),
    paste(unique(dateTime$hour), collapse = ", ")
))

datatable(df, rownames=FALSE, options = list( searching = FALSE, pageLength = 5 ))
```

As consequence, we removed timestamp columns (*raw_timestamp_part_1*, *raw_timestamp_part_2* and *cvtd_timestamp*) as well as columns that are used for internal control of the dataset and are not related to the movements (*X*, *new_window* and *num_window*) -- see page 16 of [slides](http://groupware.les.inf.puc-rio.br/public/2012.SBIA.Ugulino.WearableComputing-Presentation.pdf).

```{r}
columnsToRemove = c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'X', 'new_window', 'num_window')

training = training[, !(names(training) %in% columnsToRemove)]
validation = validation[, !(names(validation) %in% columnsToRemove)]
courseraValidation = courseraValidation[, !(names(courseraValidation) %in% columnsToRemove)]
```

To finish the dataset preparation, the character columns are converted to factors.

```{r}
training$user_name = as.factor(training$user_name)
training$classe = as.factor(training$classe)

validation$user_name = as.factor(validation$user_name)
validation$classe = as.factor(validation$classe)

courseraValidation$user_name = as.factor(courseraValidation$user_name)
```

At the end, `r ncol(unlabledDataset)-ncol(training)` columns were removed and the resulting structure is the following:

```{r}
datatable(data.frame(Column=sort(colnames(training))), rownames=FALSE, options = list( searching = TRUE, pageLength = 10 ))
```

---

# Creating Models

In this section we describe the creation of the models to predict the human activities. Applying the training set to the [Caret package](http://topepo.github.io/caret), we performed the following procedures:

 - Center and Scale of the numeric values as preprocessing steps;
 - 3 algorithms were tested:
    - [CART](https://topepo.github.io/caret/Tree_Based_Model.html) as the method; 
    - [Stochastic Gradient Boosting](https://topepo.github.io/caret/Boosting.html);    
    - [Random Forest](https://topepo.github.io/caret/Random_Forest.html) as the method;
 - For each algorithm a 10-fold cross with 5 repetitions were performed to select the best model based on parameter tunning;
 - The execution of the models were performed using 5 cores;
 - No down-sampling/up-sampling were applied. There is a difference on the number of classes (A:4185 B:2848 C:2567 D:2412 E:2706 ), but the imbalance is not exagerated. 

The configuration of the preprocessing, 10 fold cross validation with repetition and use of 5 cores was performed as follows.
```{r, message=FALSE}
library(doMC)
registerDoMC(cores = 5)
control = trainControl(method="repeatedcv", number=10, repeats=5, preProcOptions = c("center", "scale"))
```

## CART

```{r, message=FALSE}
rObjectFile = "rpartModel.rds"

if (file.exists(rObjectFile)) {
    cartModel = readRDS(rObjectFile)
} else {    
    cartModel = train(classe ~ ., data=training, method="rpart", trControl=control)
    saveRDS(cartModel, rObjectFile)
}
```

## Stochastic Gradient Boosting

```{r, message=FALSE}
rObjectFile = "gbmModel.rds"

if (file.exists(rObjectFile)) {
    gbmModel = readRDS(rObjectFile)
} else {
    gbmModel = train(classe ~ ., data=training, method="gbm", trControl=control)
    saveRDS(gbmModel, rObjectFile)
}
```

## Random Forest

```{r, message=FALSE}
rObjectFile = "rfModel.rds"

if (file.exists(rObjectFile)) {
    rfModel = readRDS(rObjectFile)
} else {    
    rfModel = train(classe ~ ., data=training, method="rf", trControl=control)
    saveRDS(rfModel, rObjectFile)
}
```

# Selecting and Assessing The Best Model

The main statistics from the model are summarized below.

The best model found was:
```{r}
extractModelData = function(model, modelName){

    resultsSize = length(model$results$Accuracy)

    data.frame(Model=rep(modelName, resultsSize),
               Accuracy=model$results$Accuracy,
               Kappa=model$results$Kappa,
               AccuracySD=model$results$AccuracySD,
               KappaSD=model$results$KappaSD)    
}

models = rbind(
        extractModelData(cartModel, "CART"),    
        extractModelData(gbmModel, "GBM"),
        extractModelData(rfModel, "Random Forest")
    )

datatable(models, rownames=FALSE, options = list( searching = TRUE, pageLength = 10 ))
```

The table shows that Random Forest obtained the best model, including the second and third places.  GBM took the second place on the overall results. The worst accuracy was from CART. The results are expected, since GBM and Random Forest ensemble results from multiple classifiers. 

## Selected Model

As result of the experiments, the model chose was Random Forest.

```{r}
bestModel = rfModel
bestModel
```

To estimate the accuracy of the best model, we use our validation set, **created from the training set at the begining of the report**. Since it was not used during the training, we expect to evaluate the accuracy on unseen data.

## Validating The Selected Model

```{r}
predictedValues = predict(bestModel, newdata = validation)
confMatrix = confusionMatrix(predictedValues, testing$classe)
confMatrix
```

As result, we estimate that the accuracy of our model fit is about 99%, while the concordance (Kappa index) was about 98.9%. The Confidence Interval was 95% (0.9973 to 0.9996).

## Check Out of Sample Error

The out of sample error rate can be calculated as follows:

```{r}
outOfSample = 1.00 - sum(diag(confMatrix$table)) / sum(confMatrix$table)
outOfSample
```

This model was used to answer the part 1 of the Course Project and obtained 100% of the predictions using the *courseraValidation* set.
